# Cloudflare D1 Database Plan for Perfect Pitch

## 1. Introduction

This document outlines the plan for migrating the Perfect Pitch application's persistence layer from SQLite within Durable Objects to Cloudflare D1. Cloudflare D1 is a serverless SQL database built on SQLite, offering better scalability, manageability, and integration within the Cloudflare ecosystem. This migration aims to provide a more robust and centralized data storage solution for the application.

**Benefits of Cloudflare D1 for Perfect Pitch:**

- **Scalability:** D1 is designed to scale with your application needs.
- **Serverless:** Integrates seamlessly with Cloudflare Workers, reducing operational overhead.
- **Centralized Data:** Consolidates data from potentially many Durable Object instances into a single, queryable database.
- **Familiar SQL:** Uses SQLite syntax, making it accessible for developers familiar with SQL.
- **Managed Service:** Cloudflare handles the underlying infrastructure, backups (Time Travel), and maintenance.

## 2. Research & Best Practices

**IMPORTANT NOTE ON PENDING RESEARCH:** The successful and optimal implementation of the strategies outlined in this plan, particularly concerning best practices for Cloudflare D1 and SQLite, is contingent upon a dedicated research phase. This research should validate assumptions, confirm specific technical choices, and refine implementation details. Proceeding with implementation without completing this research may lead to suboptimal outcomes or unforeseen challenges.
Due to limitations in the current operational mode, direct research via MCP tools (like Perplexity) could not be performed for this plan. This section, therefore, relies on general SQL database design principles, Cloudflare D1 documentation, and SQLite best practices relevant to an application like Perfect Pitch.

**Key Cloudflare D1 & SQLite Best Practices:**

- **Normalization:** Design your schema to reduce data redundancy and improve data integrity. Use separate tables for distinct entities and link them with foreign keys.
- **Appropriate Data Types:** Use the most suitable SQLite data types for each column (TEXT, INTEGER, REAL, BLOB, NUMERIC). D1 supports standard SQLite types.
- **Primary and Foreign Keys:** Clearly define primary keys for unique row identification and foreign keys to enforce relational integrity between tables.
- **Indexing:** Create indexes on columns frequently used in `WHERE` clauses, `JOIN` conditions, and `ORDER BY` clauses to improve query performance. Especially index foreign key columns.
- **Query Patterns:** Design your schema with common query patterns in mind to optimize for read and write performance.
- **D1 Specifics:**
  - Be mindful of D1's SQLite dialect and any specific limitations or behaviors.
  - Understand D1's consistency model (strong consistency for writes, eventual consistency for reads from replicas via Time Travel, though reads within the same worker after a write are generally consistent).
  - Keep database transactions concise.
- **Wrangler for Migrations:** Utilize Wrangler CLI for schema migrations to manage database evolution systematically.

## 3. Proposed Database Schema

The following schema is proposed for the Perfect Pitch application, considering entities like Users, Interviews, Questions, Messages (Transcripts), and Feedback.

### 3.1. Entities and Relationships

- **Users:** Individuals using the platform.
- **Interviews:** Specific interview sessions conducted by users.
- **Interview_Types:** (Optional) Categories for interviews (e.g., "Technical," "Behavioral").
- **Skills:** Specific skills being assessed in interviews (e.g., "JavaScript," "System Design").
- **Interview_Skills:** A junction table to link Interviews with multiple Skills.
- **Messages:** Chronological record of interactions (user speech, AI responses) within an interview. This replaces the current `messages` table and the concept of `Transcripts`.
- **Questions:** (Optional) A pre-defined bank of questions that can be used by the AI.
- **Interview_Questions:** Specific questions asked during an interview, linking to the `Messages` table for the AI's question and the user's answer.
- **AI_Feedback:** Feedback generated by the AI on user performance, either overall or for specific questions.
- **User_Feedback:** Feedback provided by the user about their interview experience or the AI's performance.

### 3.2. Schema Definition (SQL DDL)

**Note on `Interview_Types`, `Skills`, and `Interview_Skills` tables:** The definitive schema definitions for these tables, including the related `Interview_Type_Suggested_Skills` table, are detailed in [`docs/INTERVIEW_TYPES_DB_MIGRATION_PLAN.md`](./docs/INTERVIEW_TYPES_DB_MIGRATION_PLAN.md:1). That document should be considered the primary source of truth for these specific schemas, superseding the versions presented below if discrepancies exist for these particular tables.

```sql
-- Users Table
CREATE TABLE Users (
    user_id TEXT PRIMARY KEY, -- Consider UUID
    username TEXT UNIQUE NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP NOT NULL,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP NOT NULL
);

-- Interview_Types Table (Optional, if categorizing interviews)
CREATE TABLE Interview_Types (
    interview_type_id INTEGER PRIMARY KEY AUTOINCREMENT,
    type_name TEXT UNIQUE NOT NULL,
    description TEXT
);

-- Interviews Table
CREATE TABLE Interviews (
    interview_id TEXT PRIMARY KEY, -- Consider UUID
    user_id TEXT NOT NULL,
    title TEXT,
    status TEXT NOT NULL CHECK(status IN ('pending', 'in_progress', 'completed', 'cancelled', 'error')), -- e.g., pending, in_progress, completed
    interview_type_id INTEGER, -- Optional
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP NOT NULL,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP NOT NULL,
    started_at TIMESTAMP,
    completed_at TIMESTAMP,
    FOREIGN KEY (user_id) REFERENCES Users(user_id) ON DELETE CASCADE,
    FOREIGN KEY (interview_type_id) REFERENCES Interview_Types(interview_type_id) ON DELETE SET NULL
);
CREATE INDEX idx_interviews_user_id ON Interviews(user_id);
CREATE INDEX idx_interviews_status ON Interviews(status);

-- Skills Table
CREATE TABLE Skills (
    skill_id INTEGER PRIMARY KEY AUTOINCREMENT,
    skill_name TEXT UNIQUE NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP NOT NULL
);

-- Interview_Skills Junction Table (Many-to-Many for Interviews and Skills)
CREATE TABLE Interview_Skills (
    interview_id TEXT NOT NULL,
    skill_id INTEGER NOT NULL,
    PRIMARY KEY (interview_id, skill_id),
    FOREIGN KEY (interview_id) REFERENCES Interviews(interview_id) ON DELETE CASCADE,
    FOREIGN KEY (skill_id) REFERENCES Skills(skill_id) ON DELETE CASCADE
);

-- Messages Table (Replaces Transcripts)
CREATE TABLE Messages (
    message_id TEXT PRIMARY KEY, -- Consider UUID
    interview_id TEXT NOT NULL,
    user_id TEXT, -- Can be NULL if system/AI message not directly from user
    role TEXT NOT NULL CHECK(role IN ('user', 'assistant', 'system')), -- user, assistant, system
    content TEXT NOT NULL,
    timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP NOT NULL,
    audio_url TEXT, -- If storing reference to audio blobs
    processing_status TEXT CHECK(processing_status IN ('pending', 'processing', 'completed', 'error')), -- For user messages needing transcription/AI processing
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP NOT NULL,
    FOREIGN KEY (interview_id) REFERENCES Interviews(interview_id) ON DELETE CASCADE,
    FOREIGN KEY (user_id) REFERENCES Users(user_id) ON DELETE SET NULL
);
CREATE INDEX idx_messages_interview_id ON Messages(interview_id);
CREATE INDEX idx_messages_timestamp ON Messages(timestamp);

-- Questions Table (Optional: A general bank of questions)
CREATE TABLE Questions (
    question_id INTEGER PRIMARY KEY AUTOINCREMENT,
    skill_id INTEGER,
    question_text TEXT NOT NULL,
    difficulty_level TEXT CHECK(difficulty_level IN ('easy', 'medium', 'hard')),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP NOT NULL,
    FOREIGN KEY (skill_id) REFERENCES Skills(skill_id) ON DELETE SET NULL
);

-- Interview_Questions Table (Specific questions asked in an interview and their context)
CREATE TABLE Interview_Questions (
    interview_question_id TEXT PRIMARY KEY, -- Consider UUID
    interview_id TEXT NOT NULL,
    question_bank_id INTEGER, -- FK to Questions table, if applicable
    question_text_snapshot TEXT NOT NULL, -- Actual text of the question asked
    order_in_interview INTEGER,
    timestamp_asked TIMESTAMP DEFAULT CURRENT_TIMESTAMP NOT NULL,
    ai_question_message_id TEXT UNIQUE, -- FK to the AI's message in Messages table
    user_answer_message_id TEXT UNIQUE, -- FK to the User's answer message in Messages table
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP NOT NULL,
    FOREIGN KEY (interview_id) REFERENCES Interviews(interview_id) ON DELETE CASCADE,
    FOREIGN KEY (question_bank_id) REFERENCES Questions(question_id) ON DELETE SET NULL,
    FOREIGN KEY (ai_question_message_id) REFERENCES Messages(message_id) ON DELETE SET NULL,
    FOREIGN KEY (user_answer_message_id) REFERENCES Messages(message_id) ON DELETE SET NULL
);
CREATE INDEX idx_interview_questions_interview_id ON Interview_Questions(interview_id);

-- AI_Feedback Table
CREATE TABLE AI_Feedback (
    ai_feedback_id TEXT PRIMARY KEY, -- Consider UUID
    interview_id TEXT NOT NULL,
    interview_question_id TEXT, -- Link to a specific question/answer if applicable
    overall_feedback BOOLEAN DEFAULT FALSE, -- True if this is summary feedback for the whole interview
    feedback_text TEXT NOT NULL,
    rating_score INTEGER, -- e.g., 1-5
    areas_for_improvement TEXT,
    strengths TEXT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP NOT NULL,
    FOREIGN KEY (interview_id) REFERENCES Interviews(interview_id) ON DELETE CASCADE,
    FOREIGN KEY (interview_question_id) REFERENCES Interview_Questions(interview_question_id) ON DELETE SET NULL
);
CREATE INDEX idx_ai_feedback_interview_id ON AI_Feedback(interview_id);

-- User_Feedback Table
CREATE TABLE User_Feedback (
    user_feedback_id TEXT PRIMARY KEY, -- Consider UUID
    interview_id TEXT NOT NULL,
    user_id TEXT NOT NULL,
    rating_experience INTEGER, -- e.g., 1-5 for overall experience
    rating_ai_quality INTEGER, -- e.g., 1-5 for AI performance
    comments TEXT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP NOT NULL,
    FOREIGN KEY (interview_id) REFERENCES Interviews(interview_id) ON DELETE CASCADE,
    FOREIGN KEY (user_id) REFERENCES Users(user_id) ON DELETE CASCADE
);
CREATE INDEX idx_user_feedback_interview_id ON User_Feedback(interview_id);
CREATE INDEX idx_user_feedback_user_id ON User_Feedback(user_id);

-- Trigger to update 'updated_at' timestamps (Example for Users table)
-- D1 does not support complex triggers like some other SQL dbs directly in CREATE TABLE.
-- These need to be created separately if desired, or managed at the application layer.
-- For simplicity, 'updated_at' can be set by the application logic.
/*
CREATE TRIGGER update_users_updated_at
AFTER UPDATE ON Users
FOR EACH ROW
BEGIN
    UPDATE Users SET updated_at = CURRENT_TIMESTAMP WHERE user_id = OLD.user_id;
END;
*/
-- Similar triggers would be needed for other tables or handled by application logic.
-- Cloudflare D1 currently has limited trigger support. Application-level updates for `updated_at` are more reliable.
```

### 3.3. Relationships Diagram (Mermaid ERD)

```mermaid
erDiagram
    Users ||--o{ Interviews : "conducts"
    Users ||--o{ Messages : "sends (nullable user_id for system/AI)"
    Users ||--o{ User_Feedback : "provides"

    Interview_Types ||--o{ Interviews : "categorizes (optional)"

    Interviews ||--o{ Messages : "contains"
    Interviews ||--o{ Interview_Skills : "focuses_on"
    Interviews ||--o{ Interview_Questions : "includes"
    Interviews ||--o{ AI_Feedback : "receives_ai"
    Interviews ||--o{ User_Feedback : "receives_user"

    Skills ||--o{ Interview_Skills : "is_skill_for"
    Skills ||--o{ Questions : "relates_to (optional)"

    Questions ||--o{ Interview_Questions : "is_instance_of (optional)"

    Messages }o--|| Interview_Questions : "can_be_ai_question (unique)"
    Messages }o--|| Interview_Questions : "can_be_user_answer (unique)"


    Users {
        TEXT user_id PK
        TEXT username UK
        TIMESTAMP created_at
        TIMESTAMP updated_at
    }
    Interview_Types {
        INTEGER interview_type_id PK
        TEXT type_name UK
        TEXT description
    }
    Interviews {
        TEXT interview_id PK
        TEXT user_id FK
        TEXT title
        TEXT status
        INTEGER interview_type_id FK
        TIMESTAMP created_at
        TIMESTAMP updated_at
        TIMESTAMP started_at
        TIMESTAMP completed_at
    }
    Skills {
        INTEGER skill_id PK
        TEXT skill_name UK
        TIMESTAMP created_at
    }
    Interview_Skills {
        TEXT interview_id PK FK
        INTEGER skill_id PK FK
    }
    Messages {
        TEXT message_id PK
        TEXT interview_id FK
        TEXT user_id FK
        TEXT role
        TEXT content
        TIMESTAMP timestamp
        TEXT audio_url
        TEXT processing_status
        TIMESTAMP created_at
    }
    Questions {
        INTEGER question_id PK
        INTEGER skill_id FK
        TEXT question_text
        TEXT difficulty_level
        TIMESTAMP created_at
    }
    Interview_Questions {
        TEXT interview_question_id PK
        TEXT interview_id FK
        INTEGER question_bank_id FK
        TEXT question_text_snapshot
        INTEGER order_in_interview
        TIMESTAMP timestamp_asked
        TEXT ai_question_message_id FK UK
        TEXT user_answer_message_id FK UK
        TIMESTAMP created_at
    }
    AI_Feedback {
        TEXT ai_feedback_id PK
        TEXT interview_id FK
        TEXT interview_question_id FK
        BOOLEAN overall_feedback
        TEXT feedback_text
        INTEGER rating_score
        TEXT areas_for_improvement
        TEXT strengths
        TIMESTAMP created_at
    }
    User_Feedback {
        TEXT user_feedback_id PK
        TEXT interview_id FK
        TEXT user_id FK
        INTEGER rating_experience
        INTEGER rating_ai_quality
        TEXT comments
        TIMESTAMP created_at
    }
```

## 4. Implementation Steps

1.  **Setup Cloudflare D1 Database:**

    - Create the D1 database using Wrangler:
      ```bash
      wrangler d1 create perfect-pitch-db
      ```
    - Note the `database_id` and `database_name` from the output.

2.  **Configure `wrangler.jsonc`:**

    - Add the D1 database binding to your [`wrangler.jsonc`](../wrangler.jsonc) file:
      ```json
      {
        // ... other configurations
        "d1_databases": [
          {
            "binding": "DB", // This is how you'll access it in your Worker (e.g., env.DB)
            "database_name": "perfect-pitch-db", // Name used in wrangler commands
            "database_id": "<your-database-id-from-previous-step>"
          }
        ]
        // ... other configurations like "vars", "durable_objects"
      }
      ```
    - Ensure this is added correctly within the JSON structure, potentially inside a specific environment configuration if you use multiple environments (e.g., `dev`, `production`).

3.  **Schema Definition & Migration:**

    - Create a directory for migrations, e.g., `migrations/`.
    - Create an initial SQL migration file, e.g., `migrations/0000_initial_schema.sql`, containing all the `CREATE TABLE` and `CREATE INDEX` statements from section 3.2.
    - Apply the migration using Wrangler:

      ```bash
      # For local testing (creates a .wrangler/state/d1/DB.sqlite3 file)
      wrangler d1 execute perfect-pitch-db --local --file=./migrations/0000_initial_schema.sql

      # For production/preview (ensure you target the correct environment if applicable)
      wrangler d1 execute perfect-pitch-db --file=./migrations/0000_initial_schema.sql
      ```

    - For future schema changes, create new numbered migration files (e.g., `migrations/0001_add_new_feature.sql`) and apply them sequentially.

4.  **Update Application Code ([`src/services/InterviewDatabaseService.ts`](../src/services/InterviewDatabaseService.ts:1)):**

    - Refactor the existing `InterviewDatabaseService` to use the D1 binding (`c.env.DB` in Hono routes, or `this.env.DB` within a Durable Object if the service is instantiated there with access to env).
    - Replace `state.storage.sql` calls with D1 client methods.
    - Example D1 query:

      ```typescript
      // In a Hono handler or a class with access to env
      // const { results } = await env.DB.prepare("SELECT * FROM Users WHERE user_id = ?")
      //   .bind(userId)
      //   .all();

      // const stmt = env.DB.prepare("INSERT INTO Users (user_id, username) VALUES (?, ?)");
      // await stmt.bind(newUserId, newUsername).run();
      ```

    - Update all CRUD (Create, Read, Update, Delete) operations to align with the new D1 schema and D1's API.
    - Utilize prepared statements for all queries to prevent SQL injection and improve performance.
    - Handle `updated_at` timestamps at the application level when records are modified.

5.  **Data Migration (from existing DO SQLite to D1):**

    - **Assessment:** The current architecture ([`docs/ARCHITECTURE.md`](./docs/ARCHITECTURE.md)) mentions SQLite is used implicitly via `state.storage.sql` within Durable Objects. Migrating this sharded data is complex.
    - **Challenges:**
      - No central registry of all active Durable Object IDs.
      - Data is isolated within each DO instance.
    - **Possible Strategies:**
      1.  **Manual/Scripted Export (if feasible):**
          - If the number of DOs/users/interviews is small, a script could potentially be developed. This would involve:
            - Identifying DO IDs (this is the hardest part; might require iterating known IDs or having an existing list).
            - Adding a temporary method to the `Interview` DO to export its SQLite data (e.g., as JSON).
            - A Worker or local script calls this method for each DO and inserts the transformed data into D1.
      2.  **Start Fresh for New Data:**
          - Given the complexity, if the existing data is not extensive or critical for long-term retention in its current form, the most pragmatic approach might be to use D1 for all _new_ interviews and user interactions.
          - Existing user `username` cookies could be used to populate the new `Users` table in D1 upon their next login.
    - **Recommendation:** For this plan, assume D1 will be used for new data. If migration of existing data from DOs is critical, it should be treated as a separate, dedicated sub-project due to its complexity.

- **Clarification on "Start Fresh" Strategy**: It is important to reiterate that the current scope of this D1 migration plan is to use D1 for **new data only**. Any migration of **existing data** from Durable Object SQLite (including historical interviews, messages, user data, and old `interview_skills` table contents) is considered **out of scope** for this initial D1 implementation. If migration of historical data is required, it must be planned and executed as a separate, dedicated sub-project, taking into account the complexities of extracting and transforming sharded data from Durable Objects. This "start fresh" approach for D1 ensures a clear path for new data persistence while deferring the complexities of historical data migration.

6.  **Testing:**
    - **Local Development:** Use `wrangler dev --local` which emulates D1 locally. Test all database interactions thoroughly.
    - **Staging/Preview:** Deploy to a Cloudflare preview environment and test against a staging D1 instance.
    - **End-to-End Testing:** Verify all application flows that interact with the database.

## 5. Potential Challenges & Considerations

- **Data Migration Complexity:** As highlighted, migrating existing sharded data from Durable Objects is a significant challenge.
- **D1 Limits & Quotas:** Be aware of D1's documented limits (e.g., database size, query execution time, operations per second, concurrent connections). Design schema and queries efficiently.
- **Consistency Model:** D1 provides strong consistency for writes. Reads from replicas (used for Time Travel or potentially for geographic distribution if that becomes a feature) are eventually consistent. For most application flows where reads follow writes within the same Worker execution, data will be consistent.
- **Query Performance:**
  - Complex JOINs or queries on very large datasets can be slow. Optimize queries and ensure proper indexing.
  - Analyze common query patterns and optimize for them.
- **Schema Evolution:** Managing schema changes requires discipline. Use Wrangler's migration capabilities and version control your migration files.
- **Local Development vs. Cloud:** While Wrangler's local D1 emulation is good, there might be subtle differences compared to the cloud environment. Test in a preview/staging environment.
- **Error Handling:** Implement robust error handling for database operations, including handling D1-specific errors.
- **Security:**
  - Continue using prepared statements to prevent SQL injection.
  - Access to D1 is controlled via Worker bindings; ensure your Worker code and environment variable management are secure.
- **Cost:** Monitor D1 usage (storage, reads, writes, Time Travel storage) against Cloudflare's pricing model.
- **Impact on Durable Objects:** The `InterviewDatabaseService` will shift from using `state.storage.sql` to `env.DB`. If this service is instantiated or used within a Durable Object, the DO needs access to the `env.DB` binding. This might involve passing the binding during DO instantiation or refactoring where the database service is invoked.
- **Trigger Limitations:** D1 has limited support for complex database triggers. Logic like updating `updated_at` timestamps is often best handled at the application layer.
- **Backup and Restore:** D1 offers "Time Travel" for point-in-time recovery, which is a form of backup. Understand its capabilities and limitations.

This plan provides a foundational strategy for integrating Cloudflare D1 into the Perfect Pitch application. Each step, particularly data migration and application code updates, will require careful implementation and thorough testing.
